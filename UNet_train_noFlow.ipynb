{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jansp/miniconda3/envs/colorize/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/jansp/miniconda3/envs/colorize/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from tqdm import tqdm\n",
    "\n",
    "import skimage.io\n",
    "import skimage.color\n",
    "import skimage.exposure\n",
    "\n",
    "\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation and Training of VideoColorizer UNet model\n",
    "\n",
    "First without optical flow\n",
    "\n",
    "\n",
    "Inputs:\n",
    "- previous frame: L, ab channels\n",
    "- current frame: L channel\n",
    "\n",
    "Outputs:\n",
    "- current frame: ab channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAVIS Dataset path specification\n",
    "\n",
    "dataset_path = '~/Documents/Colorization/Datasets/'\n",
    "dataset_name = 'DAVIS'\n",
    "dataset_path = os.path.expanduser(dataset_path)\n",
    "dataset_path = os.path.join(dataset_path, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify input image size\n",
    "\n",
    "input_res = '180p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greyscale_path = os.path.join(dataset_path, 'JPEGImages', input_res+'_gray')\n",
    "color_path = os.path.join(dataset_path, 'JPEGImages', input_res+)\n",
    "flow_path = os.path.join(dataset_path, 'JPEGImages', input_res+'_deepflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNet architecture\n",
    "# The UNet architecture consists of an encoder and a decoder.\n",
    "# code repurposed from https://towardsdatascience.com/cook-your-first-u-net-in-pytorch-b3297a844cf3\n",
    "# https://github.com/Mostafa-wael/U-Net-in-PyTorch/tree/main\n",
    "\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 320 x 176 x 4\n",
    "        # F0: color (LAB), F1: grayscale (L)       [+later: flow (UV)]\n",
    "        self.e11 = nn.Conv2d(4, 64, kernel_size=3, padding='same') # output: 320x176x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding='same') # output: 320x176x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 320x176x64\n",
    "\n",
    "        # input: 320x176x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding='same') # output: 160x88x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding='same') # output: 160x88x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 80x44x128\n",
    "\n",
    "        # input: 80x44x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding='same') # output: 80x44x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding='same') # output: 80x44x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 40x22x256\n",
    "\n",
    "        # input: 40x22x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding='same') # output: 40x22x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding='same') # output: 40x22x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 20x11x512\n",
    "\n",
    "        # input: 20x11x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding='same') # output: 20x11x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding='same') # output: 20x11x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2) # output: 40x22x512\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding='same')\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding='same')\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2) # output: 80x44x256\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding='same')\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding='same')\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2) # output: 160x88x128\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding='same')\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2) # output: 320x176x64\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding='same')\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding='same')\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1) # output: 320x176x n_class\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "        \n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colorize",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
